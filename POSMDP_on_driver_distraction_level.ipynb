{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba38ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, binom\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.special import logsumexp, softmax\n",
    "from scipy.optimize import minimize, Bounds, LinearConstraint\n",
    "import itertools\n",
    "from itertools import combinations,permutations\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.linalg import cholesky\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class HiddenSemiMarkovDDC:\n",
    "    def __init__(self, transition, weights,means,covariances, sojourn_time,n_actions=3):\n",
    "\n",
    "        # P(s' | s, a). an array of 2 * 2 * 3\n",
    "        self.P = transition\n",
    "\n",
    "        # P(z | s).\n",
    "        self.Q_weights = weights\n",
    "        self.Q_means = means\n",
    "        self.Q_covariances = covariances\n",
    "\n",
    "        # binomial distribution\n",
    "        self.ST1 = np.array([binom.pmf(i, 74, sojourn_time[0]) for i in range(75)])\n",
    "        self.ST2 = np.array([binom.pmf(i, 14, sojourn_time[1]) for i in range(15)])\n",
    "\n",
    "        self.S = 2\n",
    "        self.Z = 2\n",
    "        self.M = 1\n",
    "        self.A = n_actions #2,10\n",
    "\n",
    "        self.log_offset = -700\n",
    "        self.min_log_acc = -1500\n",
    "\n",
    "    def params_to_covariance_exponential(self, matrix):\n",
    "      # Ensure matrix is symmetric\n",
    "      matrix = (matrix + matrix.T) / 2\n",
    "      # Exponentiate to ensure positive definiteness\n",
    "      covariance_matrix = np.exp(matrix)\n",
    "      return covariance_matrix\n",
    "\n",
    "    # Convert parameters to covariance matrix using Cholesky decomposition\n",
    "    def params_to_covariance_cholesky(self, matrix):\n",
    "      L = matrix\n",
    "      # Ensure L is a lower triangular matrix\n",
    "      L = np.tril(L)\n",
    "      # Construct the covariance matrix\n",
    "      covariance_matrix = np.dot(L, L.T)  # LL^T is positive definite\n",
    "      return covariance_matrix\n",
    "\n",
    "    def GMM_Q(self, z):\n",
    "      Q = np.zeros((self.S,))\n",
    "      for state in range(self.S):\n",
    "        emission_prob = 0\n",
    "        for mix in range(self.M):\n",
    "          #add regularization\n",
    "          cov_mat = self.Q_covariances[state, mix] + np.eye(self.Q_covariances[state, mix].shape[0]) * np.finfo(float).eps\n",
    "          cov_mat = self.params_to_covariance_cholesky(cov_mat)#cholesky(cov_mat, lower=True)#self.params_to_covariance_exponential(cov_mat)\n",
    "          emission_prob += self.Q_weights[state, mix] * multivariate_normal.pdf(z,mean=self.Q_means[state, mix],\n",
    "                                                                          cov=cov_mat,\n",
    "                                                                          #allow_singular=True\n",
    "                                                                          )\n",
    "        Q[state] = emission_prob\n",
    "        #print('Q', Q)\n",
    "      #print('Q',Q)\n",
    "      #print('soft',softmax(Q))\n",
    "      #Q = Q / Q.sum()\n",
    "      #Q = softmax(Q)\n",
    "      return Q\n",
    "\n",
    "    def sigma(self, x0, x1, a, z):\n",
    "        Q = self.GMM_Q(z)\n",
    "        #print(Q,'sigma')\n",
    "        y = np.array([x0[0],x1[0]])\n",
    "        part1 = y @ self.P[a] @ Q\n",
    "        y = np.array([x0[1:].sum(),x1[1:].sum()])\n",
    "        part2 = y @ Q\n",
    "        return part1 + part2\n",
    "\n",
    "    def update_belief(self, x0, x1, a, z):\n",
    "        denom = max(self.sigma(x0, x1, a, z), np.finfo(float).eps)\n",
    "        Q = self.GMM_Q(z)\n",
    "        #print(Q,'belief')\n",
    "        #tau = 0\n",
    "        y = np.array([x0[0],x1[0]])\n",
    "        y1 = Q[0]* self.ST1\n",
    "        y2 = Q[1]* self.ST2\n",
    "        state_trans = y @ self.P[a]\n",
    "        tau_tran_s0 = state_trans[0]*y1\n",
    "        tau_tran_s1 = state_trans[1]*y2\n",
    "        #tau = t+1\n",
    "        bel_trans_s0 = np.append(Q[0]*x0[1:],[0])\n",
    "        bel_trans_s1 = np.append(Q[1]*x1[1:],[0])\n",
    "\n",
    "\n",
    "        s0_new = (tau_tran_s0+bel_trans_s0)/denom\n",
    "        s1_new = (tau_tran_s1+bel_trans_s1)/denom\n",
    "\n",
    "        assert denom > 0, f'denom = {denom}, ERROR! ' \\\n",
    "            f'x = {x}, a = {a}, z = {z}. P = {self.P}, Q = {Q}, ST = {self.ST}'\n",
    "        return s0_new, s1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e024275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class estimate_dynamics:\n",
    "   def __init__(self, data):\n",
    "      self.data = data\n",
    "      self.n_states = 2\n",
    "      self.n_actions = 3\n",
    "      self.n_mixtures = 1   \n",
    "      self.n_features = 2\n",
    "      self.cov_type = False\n",
    "      self.vert_sum = None\n",
    "      self.beliefs = None \n",
    "      self.sigmas = None\n",
    "\n",
    "      self.no_of_params = self.n_states * self.n_actions  # n_states* n_states* n_actions     #states\n",
    "      self.no_of_params += self.n_states * self.n_mixtures  # weights\n",
    "      self.no_of_params += self.n_states * self.n_mixtures * self.n_features  # means\n",
    "      if self.cov_type == True:\n",
    "        self.no_of_params += self.n_states * self.n_features * self.n_features  # covariances\n",
    "      else:\n",
    "        self.no_of_params += self.n_states * self.n_features  # variances\n",
    "        self.no_of_params += self.n_states\n",
    "\n",
    "   def extract_parameters(self,parameters):\n",
    "      num_params_trans_matrix = self.n_states* self.n_actions\n",
    "      num_params_Q_weights = self.n_states * self.n_mixtures\n",
    "      num_params_Q_means = self.n_states * self.n_mixtures * self.n_features\n",
    "      if self.cov_type == True:\n",
    "        #if Full type covariance\n",
    "        num_params_Q_covariances = self.n_states * self.n_mixtures* self.n_features* self.n_features\n",
    "      else:\n",
    "        #if Diag covariance\n",
    "        num_params_Q_covariances = self.n_states *self.n_mixtures* self.n_features\n",
    "        \n",
    "      start = 0\n",
    "      stop = num_params_trans_matrix\n",
    "      params_ = parameters[start:stop]\n",
    "      #print(\"params_\",params_)\n",
    "      P = np.zeros((self.n_actions, 2, 2))\n",
    "\n",
    "      # Fill P dynamically\n",
    "      for action in range(self.n_actions):\n",
    "          P[action, 0, 0] = params_[2 * action]  # Probability of staying in state 0\n",
    "          P[action, 0, 1] = 1 - params_[2 * action]  # Probability of moving to state 1\n",
    "          P[action, 1, 0] = params_[2 * action + 1]  # Probability of moving to state 0 from 1\n",
    "          P[action, 1, 1] = 1 - params_[2 * action + 1]  # Probability of staying in state 1\n",
    "          \n",
    "      start = num_params_trans_matrix\n",
    "      stop = num_params_trans_matrix+num_params_Q_weights\n",
    "      Q_weights = parameters[start:stop].reshape(self.n_states, self.n_mixtures)\n",
    "      start = stop\n",
    "      stop = start+num_params_Q_means\n",
    "      Q_means = parameters[start:stop].reshape(self.n_states, self.n_mixtures, self.n_features)\n",
    "      start = stop\n",
    "      stop = start+num_params_Q_covariances\n",
    "      if self.cov_type == True:\n",
    "        #if FULL cov\n",
    "        Q_covariances = parameters[start:stop].reshape(self.n_states, self.n_mixtures, self.n_features, self.n_features)\n",
    "      else:\n",
    "        #if Diag cov\n",
    "        variances = parameters[start:stop].reshape(self.n_states, self.n_mixtures, self.n_features)\n",
    "        # Initialize the covariance matrix with zeros\n",
    "        Q_covariances = np.zeros((self.n_states, self.n_mixtures, self.n_features, self.n_features))\n",
    "        # Fill the diagonal with variances\n",
    "        for state in range(self.n_states):\n",
    "          for mixture in range(self.n_mixtures):\n",
    "            np.fill_diagonal(Q_covariances[state, mixture], variances[state, mixture])\n",
    "      start = stop\n",
    "      ST = parameters[start:]\n",
    "\n",
    "      return P, Q_weights, Q_means, Q_covariances, ST\n",
    "\n",
    "   def constraint_eq(self,params):\n",
    "      num_params_trans_matrix = self.n_states * self.n_actions\n",
    "      num_params_Q_weights = self.n_states * self.n_mixtures\n",
    "      start = num_params_trans_matrix\n",
    "      stop = start+num_params_Q_weights\n",
    "      Q_weights = params[start:stop].reshape(self.n_states, self.n_mixtures)\n",
    "      # Constraints to ensure each row in the last dimension sums to 1\n",
    "      return np.concatenate([np.sum(Q_weights, axis=1).flatten() - 1])\n",
    "\n",
    "   def ll_dynamic(self, parameters, data, init_b0, init_b1):\n",
    "      P, Q_weights, Q_means, Q_covariances, ST = self.extract_parameters(parameters)\n",
    "      agent = HiddenSemiMarkovDDC(P, Q_weights,Q_means,Q_covariances, ST,self.n_actions)\n",
    "\n",
    "      def ll_one(sample_path, init_b0, init_b1):\n",
    "          # this function calculated log likelihood for one sample path\n",
    "          ll = 0\n",
    "          x0, x1 = init_b0, init_b1\n",
    "          #print(x)\n",
    "          for cell in sample_path:\n",
    "              a, z1,z2 = cell\n",
    "              z = np.array([z1,z2])\n",
    "              a = int(a)\n",
    "              #print('a,z',a,z)\n",
    "              sigma = max(agent.sigma(x0, x1, a, z), np.finfo(float).eps)\n",
    "              assert sigma > 0, f'probability = {sigma} negativity or zero ERROR!'\n",
    "              #assert sigma < 1, f'probability = {sigma} ERROR!'\n",
    "              ll += np.log(sigma)\n",
    "              x0, x1 = agent.update_belief(x0, x1, a, z)\n",
    "              #print(x0.shape,x1.shape)\n",
    "          return ll\n",
    "\n",
    "      ll = 0  # start calculating total log likelihood\n",
    "      for i, history in enumerate(data):  # histories w.r.t. i-th initial belief\n",
    "          ll += ll_one(history, init_b0, init_b1)\n",
    "          #print(i)\n",
    "\n",
    "      return -ll  # max: log-likelihood <=> min: -log-likelihood\n",
    "   \n",
    "   def get_belief(self,parameters, data, init_b0, init_b1):\n",
    "      P, Q_weights, Q_means, Q_covariances, ST = self.extract_parameters(parameters)\n",
    "      agent = HiddenSemiMarkovDDC(P, Q_weights,Q_means,Q_covariances, ST,self.n_actions)\n",
    "\n",
    "      def ll_one(sample_path, init_b0, init_b1):\n",
    "          # this function calculated log likelihood for one sample path\n",
    "          x0, x1 = init_b0, init_b1\n",
    "          dict_x0 = []\n",
    "          dict_x1 = []\n",
    "          dict_x0.append(sum(x0))\n",
    "          dict_x1.append(sum(x1))\n",
    "          for cell in sample_path:\n",
    "              a, z1,z2 = cell\n",
    "              z = np.array([z1,z2])\n",
    "              a = int(a)\n",
    "              sigma = max(agent.sigma(x0, x1, a, z), np.finfo(float).eps)\n",
    "              assert sigma > 0, f'probability = {sigma} negativity or zero ERROR!'\n",
    "              #assert sigma < 1, f'probability = {sigma} ERROR!'\n",
    "              x0, x1 = agent.update_belief(x0, x1, a, z)\n",
    "              dict_x0.append(sum(x0))\n",
    "              dict_x1.append(sum(x1))\n",
    "          return dict_x0,dict_x1\n",
    "\n",
    "      d_x0, d_x1 = [],[]\n",
    "      for i, history in enumerate(data):  # histories w.r.t. i-th initial belief\n",
    "          dict_x0,dict_x1 = ll_one(history, init_b0, init_b1)\n",
    "          d_x0.append(dict_x0)\n",
    "          d_x1.append(dict_x1)\n",
    "      return d_x0, d_x1\n",
    "   \n",
    "   def observation_sampler(self,agent,x0,x1,num_samples):\n",
    "      state = np.random.choice(agent.S, p=[sum(x0),sum(x1)])\n",
    "      mix = np.random.choice(agent.M, p=agent.Q_weights[state])\n",
    "      # Sample from the obs distribution\n",
    "      obs = multivariate_normal.rvs(mean=agent.Q_means[state, mix],\n",
    "                              cov=agent.Q_covariances[state, mix],\n",
    "                              size = num_samples)\n",
    "      return obs\n",
    "   \n",
    "   def compute_beliefs_and_sigmas(self,agent, num_samples=100):\n",
    "      # Create caches\n",
    "      belief_cache = {}\n",
    "      sigma_cache = {}\n",
    "\n",
    "      # Create the states\n",
    "      vertices = np.eye(90)  # 90, n_bel(or bstates)\n",
    "      vert_ = []\n",
    "      vert_sum = []\n",
    "      \n",
    "      for i in range(len(vertices)):\n",
    "          vert = vertices[i]\n",
    "          v1 = vert[:75]  # :75, n_bstates for first ST\n",
    "          v2 = vert[75:]  # 75:, n_bstates for second ST\n",
    "          vert_.append([v1, v2])\n",
    "          vert_sum.append([v1.sum(), v2.sum()])\n",
    "      \n",
    "      vert_sum = np.array(vert_sum)\n",
    "\n",
    "      # Store sigmas, beliefs\n",
    "      for x in range(len(vert_)):\n",
    "          for a in range(agent.A):\n",
    "              obs_MC = self.observation_sampler(agent,vert_[x][0], vert_[x][1], num_samples)\n",
    "              for i, z in enumerate(obs_MC):\n",
    "                  key = (tuple(vert_[x][0]) + tuple(vert_[x][1]), a, i)\n",
    "                  x_next0, x_next1 = agent.update_belief(vert_[x][0], vert_[x][1], a, z)\n",
    "                  sigma_cache[key] = agent.sigma(vert_[x][0], vert_[x][1], a, z)\n",
    "                  belief_cache[key] = np.concatenate((x_next0, x_next1))\n",
    "\n",
    "      # Get vectorized beliefs\n",
    "      keys = list(belief_cache.keys())\n",
    "      num_keys = len(keys)\n",
    "      array_shape = belief_cache[keys[0]].shape\n",
    "      \n",
    "      four_d_array = np.empty((num_keys, *array_shape))\n",
    "      for i, key in enumerate(keys):\n",
    "          four_d_array[i] = belief_cache[key]\n",
    "      \n",
    "      beliefs = four_d_array.reshape(90, self.n_actions, num_samples, 90)  # n_states, n_acts, n_obs, n_bel\n",
    "      \n",
    "      # Get vectorized sigmas\n",
    "      keys = list(sigma_cache.keys())\n",
    "      num_keys = len(keys)\n",
    "      array_shape = sigma_cache[keys[0]].shape\n",
    "      \n",
    "      four_d_array = np.empty((num_keys, *array_shape))\n",
    "      for i, key in enumerate(keys):\n",
    "          four_d_array[i] = sigma_cache[key]\n",
    "      \n",
    "      sigmas_before_norm = four_d_array.reshape(90, self.n_actions, num_samples)  # n_states, n_acts, n_obs\n",
    "      \n",
    "      # Normalize sigmas\n",
    "      sum_along_last_dim = np.sum(sigmas_before_norm, axis=2, keepdims=True)\n",
    "      sigmas = sigmas_before_norm / sum_along_last_dim\n",
    "      \n",
    "      self.vert_sum = vert_sum\n",
    "      self.beliefs = beliefs\n",
    "      self.sigmas = sigmas\n",
    "      return 0\n",
    "   \n",
    "   def value_function(self,reward_matrix,beta):\n",
    "      Q_val = self.vert_sum @ reward_matrix\n",
    "      V = logsumexp(Q_val, axis=1)\n",
    "      #count = 0\n",
    "      while True:\n",
    "          # Multiply using einsum\n",
    "          inter = np.einsum('ijkl,lm->ijkm', self.beliefs, Q_val)\n",
    "          EV = logsumexp(inter, axis=-1)\n",
    "          # Compute the dot product across the last dimension\n",
    "          future_ = np.einsum('ijk,ijk->ij', EV, self.sigmas)\n",
    "\n",
    "          Q_next = self.vert_sum @ reward_matrix + beta * future_\n",
    "          V_next = logsumexp(Q_next, axis=1)\n",
    "\n",
    "          condition = abs(V_next - V).sum()\n",
    "          Q_val = Q_next\n",
    "          V = V_next\n",
    "          if condition < 0.01:\n",
    "            break\n",
    "          #count+=1\n",
    "          #print('count',count,'condition',condition)\n",
    "\n",
    "      return Q_val, V\n",
    "   \n",
    "   def reward(self,w, B, C, s, a):\n",
    "      return w[s] * B[a] + (1 - w[s]) * (-C[a])\n",
    "   \n",
    "   def ll_ccp(self,parameters,agent, data, init_b0, init_b1, beta=0.9):\n",
    "    #r = np.zeros((2, self.n_actions))\n",
    "    # Fill the remaining elements from the vector\n",
    "    #r[0,0]=0\n",
    "    #r[1,0]=0\n",
    "    #r[:, 1:] = parameters.reshape(2, self.n_actions-1)\n",
    "    B = {0: 0,1: 4.1, 2: 5.3}\n",
    "    C = {0: 0,1: 3, 2: 6.2}\n",
    "    r = np.zeros((2, self.n_actions))\n",
    "    for s in [0, 1]:\n",
    "        for a in range(self.n_actions):\n",
    "            if a == 0:\n",
    "                r[s, a] = 0  # already set, explicitly\n",
    "            else:\n",
    "                r[s, a] = self.reward(parameters, B, C, s, a)\n",
    "    Q_val, _ = self.value_function(r, beta)\n",
    "    def ll_one(sample_path, init_b0, init_b1):\n",
    "        # this function calculated log likelihood for one sample path\n",
    "        ll = 0\n",
    "        x0, x1 = init_b0, init_b1\n",
    "        for cell in sample_path:\n",
    "            a, z1, z2 = cell\n",
    "            z = np.array([z1,z2])\n",
    "            a = int(a)\n",
    "            belief = np.concatenate((x0, x1))\n",
    "            Q_x = belief.flatten() @ Q_val\n",
    "            ccp = max(softmax(Q_x)[a], np.finfo(float).eps)\n",
    "            assert ccp > 0, f'prob = {ccp}, negativity or zero ERROR!'\n",
    "            ll += np.log(ccp)\n",
    "            x0, x1 = agent.update_belief(x0, x1, a, z)\n",
    "        return ll\n",
    "\n",
    "    ll = 0\n",
    "    for i, history in enumerate(data):  # histories w.r.t. i-th initial belief\n",
    "            ll += ll_one(history, init_b0, init_b1)\n",
    "\n",
    "    return -ll  # max: log-likelihood <=> min: -log-likelihood\n",
    "\n",
    "   def pretty_print_arrays(self,P, Q_means, Q_covariances, ST):\n",
    "      np.set_printoptions(precision=3, suppress=True)\n",
    "      print(\"State Transitions:\\n\", P, \"\\n\")\n",
    "      print(\"Means:\\n\", Q_means, \"\\n\")\n",
    "      print(\"Variance:\\n\", Q_covariances, \"\\n\")\n",
    "      print(\"Sojourn Time:\\n\", ST, \"\\n\")\n",
    "\n",
    "   def plot_distraction_estimates(self,result, df, time_start=0, time_end=2000):\n",
    "      init_b0 = np.zeros(75)\n",
    "      init_b0[64] = 0.50\n",
    "      init_b0[65] = 0.50\n",
    "      init_b1 = np.zeros(15)\n",
    "      b0,b1 = self.get_belief(result.x,self.data,init_b0, init_b1)\n",
    "      b0 = [item for sublist in b0 for item in sublist]\n",
    "      b1 = [item for sublist in b1 for item in sublist]\n",
    "      df['x0'] = b0\n",
    "      df['x1'] = b1\n",
    "      '''\n",
    "      plt.figure(figsize=(12, 6))\n",
    "      plt.plot(df['distraction'][time_start:time_end], label='distraction ground truth')\n",
    "      plt.plot(df['x0'][time_start:time_end], label='s0 estimated', color='r')\n",
    "      plt.xlabel('Time')\n",
    "      plt.ylabel('Prob')\n",
    "      plt.legend()\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "      '''\n",
    "      plt.figure(figsize=(12, 6))\n",
    "      plt.plot(df['distraction'][time_start:time_end], label='distraction ground truth')\n",
    "      plt.plot(df['x1'][time_start:time_end], label='s1 estimated', color='k')\n",
    "      plt.xlabel('Time')\n",
    "      plt.ylabel('Prob')\n",
    "      plt.legend()\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "\n",
    "      x = df['Velocity']\n",
    "      y = df['PCPS']\n",
    "      z = df['x1']\n",
    "\n",
    "      fig, ax = plt.subplots( figsize = (9,6))\n",
    "      scatter = ax.scatter(x, y, c = z , cmap = 'coolwarm' )\n",
    "      ax.set_xlabel('Velocity')\n",
    "      ax.set_ylabel('PCPS')\n",
    "      cb = fig.colorbar(scatter, ax=ax, label='Estimated State')\n",
    "      plt.show()\n",
    "\n",
    "      z = df['distraction']\n",
    "      fig, ax = plt.subplots( figsize = (9,6))\n",
    "      scatter = ax.scatter(x, y, c = z , cmap = 'coolwarm' )\n",
    "      ax.set_xlabel('Velocity')\n",
    "      ax.set_ylabel('PCPS')\n",
    "      cb = fig.colorbar(scatter, ax=ax, label='Ground truth')\n",
    "      plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c2721cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P1 ===\n",
      "Fitting the dynamics model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_slsqp_py.py:435: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  fx = wrapped_fun(x)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n\u001b[1;32m    101\u001b[0m participant_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m19\u001b[39m)]\n\u001b[0;32m--> 102\u001b[0m results_df \u001b[38;5;241m=\u001b[39m fit_models_all_participants(participant_ids, participant_bounds)\n",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m, in \u001b[0;36mfit_models_all_participants\u001b[0;34m(participant_ids, participant_bounds)\u001b[0m\n\u001b[1;32m     34\u001b[0m init_p \u001b[38;5;241m=\u001b[39m participant_bounds[participant_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     res \u001b[38;5;241m=\u001b[39m minimize(\n\u001b[1;32m     38\u001b[0m         fun\u001b[38;5;241m=\u001b[39mestimate_d\u001b[38;5;241m.\u001b[39mll_dynamic,\n\u001b[1;32m     39\u001b[0m         x0\u001b[38;5;241m=\u001b[39minit_p,\n\u001b[1;32m     40\u001b[0m         args\u001b[38;5;241m=\u001b[39m(train_data, init_b0, init_b1),\n\u001b[1;32m     41\u001b[0m         bounds\u001b[38;5;241m=\u001b[39mbnds,\n\u001b[1;32m     42\u001b[0m         constraints\u001b[38;5;241m=\u001b[39mconstraints,\n\u001b[1;32m     43\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSLSQP\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     44\u001b[0m         options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m     param_estimates_1 \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m     47\u001b[0m     LL \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_minimize.py:750\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    747\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cobyqa(fun, x0, args, bounds, constraints, callback,\n\u001b[1;32m    748\u001b[0m                            \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslsqp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 750\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[1;32m    751\u001b[0m                           constraints, callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    753\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[1;32m    754\u001b[0m                                        bounds, constraints,\n\u001b[1;32m    755\u001b[0m                                        callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_slsqp_py.py:439\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    436\u001b[0m     c \u001b[38;5;241m=\u001b[39m _eval_constraint(x, cons)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# gradient evaluation required\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     g \u001b[38;5;241m=\u001b[39m append(wrapped_grad(x), \u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    440\u001b[0m     a \u001b[38;5;241m=\u001b[39m _eval_con_normals(x, cons, la, n, m, meq, mieq)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m majiter \u001b[38;5;241m>\u001b[39m majiter_prev:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# call callback if major iteration has incremented\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_optimize.py:305\u001b[0m, in \u001b[0;36m_clip_x_for_func.<locals>.eval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meval\u001b[39m(x):\n\u001b[1;32m    304\u001b[0m     x \u001b[38;5;241m=\u001b[39m _check_clip_x(x, bounds)\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:332\u001b[0m, in \u001b[0;36mScalarFunction.grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x(x)\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:307\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_orig_grad \u001b[38;5;129;01min\u001b[39;00m FD_METHODS:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_grad(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, f0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:48\u001b[0m, in \u001b[0;36m_wrapper_grad.<locals>.wrapped1\u001b[0;34m(x, f0)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped1\u001b[39m(x, f0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     47\u001b[0m     ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m approx_derivative(\n\u001b[1;32m     49\u001b[0m         fun, x, f0\u001b[38;5;241m=\u001b[39mf0, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfinite_diff_options\n\u001b[1;32m     50\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:523\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[1;32m    524\u001b[0m                              use_one_sided, method)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:596\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    594\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n\u001b[1;32m    595\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x1[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m     df \u001b[38;5;241m=\u001b[39m fun(x1) \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    598\u001b[0m     x1[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m h[i]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_numdiff.py:474\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(x\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    472\u001b[0m     x \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(x, x0\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 474\u001b[0m f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(fun(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:21\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[2], line 104\u001b[0m, in \u001b[0;36mestimate_dynamics.ll_dynamic\u001b[0;34m(self, parameters, data, init_b0, init_b1)\u001b[0m\n\u001b[1;32m    102\u001b[0m ll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# start calculating total log likelihood\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, history \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):  \u001b[38;5;66;03m# histories w.r.t. i-th initial belief\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     ll \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ll_one(history, init_b0, init_b1)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m#print(i)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mll\n",
      "Cell \u001b[0;32mIn[2], line 98\u001b[0m, in \u001b[0;36mestimate_dynamics.ll_dynamic.<locals>.ll_one\u001b[0;34m(sample_path, init_b0, init_b1)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m#assert sigma < 1, f'probability = {sigma} ERROR!'\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     ll \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(sigma)\n\u001b[0;32m---> 98\u001b[0m     x0, x1 \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mupdate_belief(x0, x1, a, z)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m#print(x0.shape,x1.shape)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ll\n",
      "Cell \u001b[0;32mIn[1], line 86\u001b[0m, in \u001b[0;36mHiddenSemiMarkovDDC.update_belief\u001b[0;34m(self, x0, x1, a, z)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_belief\u001b[39m(\u001b[38;5;28mself\u001b[39m, x0, x1, a, z):\n\u001b[1;32m     85\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma(x0, x1, a, z), np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39meps)\n\u001b[0;32m---> 86\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGMM_Q(z)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m#print(Q,'belief')\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m#tau = 0\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([x0[\u001b[38;5;241m0\u001b[39m],x1[\u001b[38;5;241m0\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[1], line 63\u001b[0m, in \u001b[0;36mHiddenSemiMarkovDDC.GMM_Q\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     61\u001b[0m     cov_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_covariances[state, mix] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_covariances[state, mix]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m     62\u001b[0m     cov_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams_to_covariance_cholesky(cov_mat)\u001b[38;5;66;03m#cholesky(cov_mat, lower=True)#self.params_to_covariance_exponential(cov_mat)\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     emission_prob \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_weights[state, mix] \u001b[38;5;241m*\u001b[39m multivariate_normal\u001b[38;5;241m.\u001b[39mpdf(z,mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_means[state, mix],\n\u001b[1;32m     64\u001b[0m                                                                     cov\u001b[38;5;241m=\u001b[39mcov_mat,\n\u001b[1;32m     65\u001b[0m                                                                     \u001b[38;5;66;03m#allow_singular=True\u001b[39;00m\n\u001b[1;32m     66\u001b[0m                                                                     )\n\u001b[1;32m     67\u001b[0m   Q[state] \u001b[38;5;241m=\u001b[39m emission_prob\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;66;03m#print('Q', Q)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#print('Q',Q)\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#print('soft',softmax(Q))\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#Q = Q / Q.sum()\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#Q = softmax(Q)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/scipy/stats/_multivariate.py:592\u001b[0m, in \u001b[0;36mmultivariate_normal_gen.pdf\u001b[0;34m(self, x, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m    590\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_quantiles(x, dim)\n\u001b[1;32m    591\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logpdf(x, mean, cov_object))\n\u001b[0;32m--> 592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(cov_object\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m<\u001b[39m dim):\n\u001b[1;32m    593\u001b[0m     out_of_bounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mcov_object\u001b[38;5;241m.\u001b[39m_support_mask(x\u001b[38;5;241m-\u001b[39mmean)\n\u001b[1;32m    594\u001b[0m     out[out_of_bounds] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:2491\u001b[0m, in \u001b[0;36m_any_dispatcher\u001b[0;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[1;32m   2483\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(\n\u001b[1;32m   2486\u001b[0m         a, np\u001b[38;5;241m.\u001b[39madd, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, dtype, out,\n\u001b[1;32m   2487\u001b[0m         keepdims\u001b[38;5;241m=\u001b[39mkeepdims, initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere\n\u001b[1;32m   2488\u001b[0m     )\n\u001b[0;32m-> 2491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_any_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   2492\u001b[0m                     where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, where, out)\n\u001b[1;32m   2496\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_any_dispatcher)\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21many\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('participant_bounds.json', 'r') as f:\n",
    "    participant_bounds = json.load(f)\n",
    "\n",
    "for pid in participant_bounds.keys():\n",
    "    participant_bounds[pid]['lb'] = np.array(participant_bounds[pid]['lb'])\n",
    "    participant_bounds[pid]['ub'] = np.array(participant_bounds[pid]['ub'])\n",
    "    participant_bounds[pid]['init'] = np.array(participant_bounds[pid]['init'])\n",
    "\n",
    "def fit_models_all_participants(participant_ids, participant_bounds):\n",
    "    results = []\n",
    "\n",
    "    for participant_id in participant_ids:\n",
    "        print(f\"\\n=== Processing {participant_id} ===\")\n",
    "\n",
    "        # Load training data\n",
    "        try:\n",
    "            with open(f\"new/{participant_id}_train_data.pkl\", \"rb\") as f:\n",
    "                train_data = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load training data for {participant_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Fit dynamics model\n",
    "        print(\"Fitting the dynamics model...\")\n",
    "        estimate_d = estimate_dynamics(train_data)\n",
    "        constraints = {'type': 'eq', 'fun': estimate_d.constraint_eq, 'args': ()}\n",
    "        lb = participant_bounds[participant_id]['lb']\n",
    "        ub = participant_bounds[participant_id]['ub']\n",
    "        bnds = Bounds(lb=lb, ub=ub)\n",
    "\n",
    "        init_b0 = np.random.rand(75)\n",
    "        init_b0 = init_b0 / init_b0.sum()\n",
    "        init_b1 = np.zeros(15)\n",
    "        init_p = participant_bounds[participant_id]['init']\n",
    "\n",
    "        try:\n",
    "            res = minimize(\n",
    "                fun=estimate_d.ll_dynamic,\n",
    "                x0=init_p,\n",
    "                args=(train_data, init_b0, init_b1),\n",
    "                bounds=bnds,\n",
    "                constraints=constraints,\n",
    "                method='SLSQP',\n",
    "                options={'disp': 0}\n",
    "            )\n",
    "            param_estimates_1 = res.x\n",
    "            LL = res.fun\n",
    "            print(\"Dynamic model likelihood:\", LL)\n",
    "            print(\"Estimated Dynamics:\", param_estimates_1)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {participant_id} (dynamic model failed): {e}\")\n",
    "            results.append({\n",
    "                \"Participant\": participant_id,\n",
    "                \"LL\": None,\n",
    "                \"DynamicsParams\": None,\n",
    "                \"RewardParams\": None\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Fit reward model\n",
    "        print(\"Fitting the reward model...\")\n",
    "        try:\n",
    "            P, Q_weights, Q_means, Q_covariances, ST = estimate_d.extract_parameters(param_estimates_1)\n",
    "            agent = HiddenSemiMarkovDDC(np.array(P), np.array(Q_weights), np.array(Q_means),\n",
    "                                        np.array(Q_covariances), np.array(ST))\n",
    "            estimate_d.compute_beliefs_and_sigmas(agent)\n",
    "\n",
    "            b = np.random.random(75 + 15)\n",
    "            b = b / b.sum()\n",
    "            init_b0 = b[:75]\n",
    "            init_b1 = b[75:]\n",
    "\n",
    "            x0 = np.random.random(2)\n",
    "            bnds2 = Bounds([0]*2, [1]*2)\n",
    "\n",
    "            res2 = minimize(\n",
    "                fun=estimate_d.ll_ccp,\n",
    "                x0=x0,\n",
    "                args=(agent, train_data, init_b0, init_b1),\n",
    "                bounds=bnds2,\n",
    "                method='SLSQP',\n",
    "                options={'disp': 0}\n",
    "            )\n",
    "            param_estimates_2 = res2.x\n",
    "            print(\"Estimated Rewards:\", param_estimates_2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {participant_id} (reward model failed): {e}\")\n",
    "            param_estimates_2 = None\n",
    "\n",
    "        # Store result\n",
    "        results.append({\n",
    "            \"Participant\": participant_id,\n",
    "            \"LL\": LL,\n",
    "            \"DynamicsParams\": param_estimates_1,\n",
    "            \"RewardParams\": param_estimates_2\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "participant_ids = [f'P{i}' for i in range(1, 19)]\n",
    "results_df = fit_models_all_participants(participant_ids, participant_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d215a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing participant P1...\n",
      "Processing participant P2...\n",
      "Processing participant P3...\n",
      "Processing participant P4...\n",
      "Processing participant P5...\n",
      "Processing participant P6...\n",
      "Processing participant P7...\n",
      "Processing participant P8...\n",
      "Processing participant P9...\n",
      "Processing participant P10...\n",
      "Processing participant P11...\n",
      "Processing participant P12...\n",
      "Processing participant P13...\n",
      "Processing participant P14...\n",
      "Processing participant P15...\n",
      "Processing participant P16...\n",
      "Processing participant P17...\n",
      "Processing participant P18...\n",
      "        Event Type  Total Events POSMDP Model (% Detected) Baseline Rule (% Detected)\n",
      "Distraction Events            38                    100.0%                      68.4%\n",
      "Lane Offset Events            17                    100.0%                      70.6%\n",
      "\n",
      "Average Lead Time (in time steps): 1.67 seconds\n"
     ]
    }
   ],
   "source": [
    "def analyze_participants(participant_ids, mean_1_dict, input_dir=\"new\"):\n",
    "    all_results = []\n",
    "\n",
    "    for pid in participant_ids:\n",
    "        print(f\"Processing participant {pid}...\")\n",
    "\n",
    "        # Load test data\n",
    "        with open(os.path.join(input_dir, f\"{pid}_test_data.pkl\"), \"rb\") as f:\n",
    "            test_data = pickle.load(f)\n",
    "\n",
    "        #initialize class\n",
    "        estimate_d = estimate_dynamics(test_data)\n",
    "        # Use pretrained values for participant\n",
    "\n",
    "        mean_1 = mean_1_dict[pid]\n",
    "\n",
    "        # Initialize beliefs\n",
    "        init_b0 = np.random.rand(75)\n",
    "        init_b0 /= init_b0.sum()\n",
    "        init_b1 = np.zeros(15)\n",
    "\n",
    "        # Estimate beliefs\n",
    "        b0, b1 = estimate_d.get_belief(mean_1, test_data, init_b0, init_b1)\n",
    "        b0 = [item for sublist in b0 for item in sublist]\n",
    "        b1 = [item for sublist in b1 for item in sublist]\n",
    "\n",
    "        # Load true data\n",
    "        df = pd.read_csv(os.path.join(input_dir, f\"{pid}_true_data.csv\"))\n",
    "        df['x0'] = b0\n",
    "        df['x1'] = b1\n",
    "\n",
    "        # Extract segments\n",
    "        segments = []\n",
    "        current_segment = []\n",
    "        for _, row in df.iterrows():\n",
    "            if row['distraction'] == 1:\n",
    "                current_segment.append([row['x1'], row['glances_greater_than_2_mean'], abs(row['LaneOffset'])])\n",
    "            else:\n",
    "                if current_segment:\n",
    "                    segments.append(current_segment)\n",
    "                    current_segment = []\n",
    "        if current_segment:\n",
    "            segments.append(current_segment)\n",
    "\n",
    "        segments_array = [np.array(seg) for seg in segments]\n",
    "\n",
    "        result = []\n",
    "        for segment in segments_array:\n",
    "            cond0 = np.any(segment[:, 0] >= 0.5)\n",
    "            cond1 = np.any(segment[:, 1] > 0)\n",
    "            cond2 = np.any(segment[:, 2] >= 0.9)\n",
    "            result.append([cond0, cond1, cond2])\n",
    "        result_array = np.array(result)\n",
    "\n",
    "        # Summary statistics\n",
    "        n_true_distractions = len(result_array)\n",
    "        n_predicted_distractions = int(np.sum(result_array[:, 0]))\n",
    "        n_threshold_distractions = int(np.sum(result_array[:, 1]))\n",
    "        n_true_lane_offsets = int(np.sum(result_array[:, 2]))\n",
    "        n_lane_model = int(np.sum(result_array[:, 0] & result_array[:, 2]))\n",
    "        n_lane_thresh = int(np.sum(result_array[:, 1] & result_array[:, 2]))\n",
    "\n",
    "        # Lead time analysis\n",
    "        lead_times = []\n",
    "        for segment in segments_array:\n",
    "            col0_pred = np.where(segment[:, 0] >= 0.5)[0]\n",
    "            col1_pred = np.where(segment[:, 1] > 0)[0]\n",
    "            if len(col0_pred) > 0 and len(col1_pred) > 0:\n",
    "                idx_0 = col0_pred[0]\n",
    "                idx_1 = col1_pred[0]\n",
    "                lead_times.append(idx_1 - idx_0)\n",
    "\n",
    "        if lead_times:\n",
    "            lead_time_mean = np.mean(lead_times)\n",
    "        else:\n",
    "            lead_time_mean = None\n",
    "\n",
    "        all_results.append({\n",
    "            \"Participant\": pid,\n",
    "            \"TrueDistractionEvents\": n_true_distractions,\n",
    "            \"ModelDistractionEvents\": n_predicted_distractions,\n",
    "            \"ThresholdDistractionEvents\": n_threshold_distractions,\n",
    "            \"TrueLaneOffsetEvents\": n_true_lane_offsets,\n",
    "            \"ModelLaneOffsetEvents\": n_lane_model,\n",
    "            \"ThresholdLaneOffsetEvents\": n_lane_thresh,\n",
    "            \"LeadTimeMean\": lead_time_mean,\n",
    "        })\n",
    "\n",
    "    # Compile into DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    return results_df\n",
    "\n",
    "with open(\"pre_trained_values.pkl\", \"rb\") as f:\n",
    "    pre_trained_dict = pickle.load(f)\n",
    "participant_ids = [f\"P{i}\" for i in range(1, 19)]\n",
    "results_df = analyze_participants(participant_ids, pre_trained_dict)\n",
    "\n",
    "def print_detection_summary(results_df):\n",
    "    # Aggregate totals\n",
    "    total_distraction = results_df[\"TrueDistractionEvents\"].sum()\n",
    "    model_distraction = results_df[\"ModelDistractionEvents\"].sum()\n",
    "    rule_distraction = results_df[\"ThresholdDistractionEvents\"].sum()\n",
    "\n",
    "    total_lane = results_df[\"TrueLaneOffsetEvents\"].sum()\n",
    "    model_lane = results_df[\"ModelLaneOffsetEvents\"].sum()\n",
    "    rule_lane = results_df[\"ThresholdLaneOffsetEvents\"].sum()\n",
    "\n",
    "    # Average lead time (excluding NaNs)\n",
    "    avg_lead_time = results_df[\"LeadTimeMean\"].dropna().mean()\n",
    "\n",
    "    # Create summary table\n",
    "    summary = pd.DataFrame([\n",
    "        {\n",
    "            \"Event Type\": \"Distraction Events\",\n",
    "            \"Total Events\": total_distraction,\n",
    "            \"POSMDP Model (% Detected)\": f\"{(model_distraction / total_distraction * 100):.1f}%\",\n",
    "            \"Baseline Rule (% Detected)\": f\"{(rule_distraction / total_distraction * 100):.1f}%\"\n",
    "        },\n",
    "        {\n",
    "            \"Event Type\": \"Lane Offset Events\",\n",
    "            \"Total Events\": total_lane,\n",
    "            \"POSMDP Model (% Detected)\": f\"{(model_lane / total_lane * 100):.1f}%\",\n",
    "            \"Baseline Rule (% Detected)\": f\"{(rule_lane / total_lane * 100):.1f}%\"\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    print(summary.to_string(index=False))\n",
    "    print(f\"\\nAverage Lead Time (in time steps): {avg_lead_time:.2f} seconds\")\n",
    "    \n",
    "print_detection_summary(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
