{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba38ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, binom\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.special import logsumexp, softmax\n",
    "from scipy.optimize import minimize, Bounds, LinearConstraint\n",
    "import itertools\n",
    "from itertools import combinations,permutations\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.linalg import cholesky\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class HiddenSemiMarkovDDC:\n",
    "    def __init__(self, transition, weights,means,covariances, sojourn_time,n_actions=3):\n",
    "\n",
    "        # P(s' | s, a). an array of 2 * 2 * 3\n",
    "        self.P = transition\n",
    "\n",
    "        # P(z | s).\n",
    "        self.Q_weights = weights\n",
    "        self.Q_means = means\n",
    "        self.Q_covariances = covariances\n",
    "\n",
    "        # binomial distribution\n",
    "        self.ST1 = np.array([binom.pmf(i, 74, sojourn_time[0]) for i in range(75)])\n",
    "        self.ST2 = np.array([binom.pmf(i, 14, sojourn_time[1]) for i in range(15)])\n",
    "\n",
    "        self.S = 2\n",
    "        self.Z = 2\n",
    "        self.M = 1\n",
    "        self.A = n_actions #2,10\n",
    "\n",
    "        self.log_offset = -700\n",
    "        self.min_log_acc = -1500\n",
    "\n",
    "    def params_to_covariance_exponential(self, matrix):\n",
    "      # Ensure matrix is symmetric\n",
    "      matrix = (matrix + matrix.T) / 2\n",
    "      # Exponentiate to ensure positive definiteness\n",
    "      covariance_matrix = np.exp(matrix)\n",
    "      return covariance_matrix\n",
    "\n",
    "    # Convert parameters to covariance matrix using Cholesky decomposition\n",
    "    def params_to_covariance_cholesky(self, matrix):\n",
    "      L = matrix\n",
    "      # Ensure L is a lower triangular matrix\n",
    "      L = np.tril(L)\n",
    "      # Construct the covariance matrix\n",
    "      covariance_matrix = np.dot(L, L.T)  # LL^T is positive definite\n",
    "      return covariance_matrix\n",
    "\n",
    "    def GMM_Q(self, z):\n",
    "      Q = np.zeros((self.S,))\n",
    "      for state in range(self.S):\n",
    "        emission_prob = 0\n",
    "        for mix in range(self.M):\n",
    "          #add regularization\n",
    "          cov_mat = self.Q_covariances[state, mix] + np.eye(self.Q_covariances[state, mix].shape[0]) * np.finfo(float).eps\n",
    "          cov_mat = self.params_to_covariance_cholesky(cov_mat)#cholesky(cov_mat, lower=True)#self.params_to_covariance_exponential(cov_mat)\n",
    "          emission_prob += self.Q_weights[state, mix] * multivariate_normal.pdf(z,mean=self.Q_means[state, mix],\n",
    "                                                                          cov=cov_mat,\n",
    "                                                                          #allow_singular=True\n",
    "                                                                          )\n",
    "        Q[state] = emission_prob\n",
    "        #print('Q', Q)\n",
    "      #print('Q',Q)\n",
    "      #print('soft',softmax(Q))\n",
    "      #Q = Q / Q.sum()\n",
    "      #Q = softmax(Q)\n",
    "      return Q\n",
    "\n",
    "    def sigma(self, x0, x1, a, z):\n",
    "        Q = self.GMM_Q(z)\n",
    "        #print(Q,'sigma')\n",
    "        y = np.array([x0[0],x1[0]])\n",
    "        part1 = y @ self.P[a] @ Q\n",
    "        y = np.array([x0[1:].sum(),x1[1:].sum()])\n",
    "        part2 = y @ Q\n",
    "        return part1 + part2\n",
    "\n",
    "    def update_belief(self, x0, x1, a, z):\n",
    "        denom = max(self.sigma(x0, x1, a, z), np.finfo(float).eps)\n",
    "        Q = self.GMM_Q(z)\n",
    "        #print(Q,'belief')\n",
    "        #tau = 0\n",
    "        y = np.array([x0[0],x1[0]])\n",
    "        y1 = Q[0]* self.ST1\n",
    "        y2 = Q[1]* self.ST2\n",
    "        state_trans = y @ self.P[a]\n",
    "        tau_tran_s0 = state_trans[0]*y1\n",
    "        tau_tran_s1 = state_trans[1]*y2\n",
    "        #tau = t+1\n",
    "        bel_trans_s0 = np.append(Q[0]*x0[1:],[0])\n",
    "        bel_trans_s1 = np.append(Q[1]*x1[1:],[0])\n",
    "\n",
    "\n",
    "        s0_new = (tau_tran_s0+bel_trans_s0)/denom\n",
    "        s1_new = (tau_tran_s1+bel_trans_s1)/denom\n",
    "\n",
    "        assert denom > 0, f'denom = {denom}, ERROR! ' \\\n",
    "            f'x = {x}, a = {a}, z = {z}. P = {self.P}, Q = {Q}, ST = {self.ST}'\n",
    "        return s0_new, s1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e024275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class estimate_dynamics:\n",
    "   def __init__(self, data):\n",
    "      self.data = data\n",
    "      self.n_states = 2\n",
    "      self.n_actions = 3\n",
    "      self.n_mixtures = 1   \n",
    "      self.n_features = 2\n",
    "      self.cov_type = False\n",
    "      self.vert_sum = None\n",
    "      self.beliefs = None \n",
    "      self.sigmas = None\n",
    "\n",
    "      self.no_of_params = self.n_states * self.n_actions  # n_states* n_states* n_actions     #states\n",
    "      self.no_of_params += self.n_states * self.n_mixtures  # weights\n",
    "      self.no_of_params += self.n_states * self.n_mixtures * self.n_features  # means\n",
    "      if self.cov_type == True:\n",
    "        self.no_of_params += self.n_states * self.n_features * self.n_features  # covariances\n",
    "      else:\n",
    "        self.no_of_params += self.n_states * self.n_features  # variances\n",
    "        self.no_of_params += self.n_states\n",
    "\n",
    "   def extract_parameters(self,parameters):\n",
    "      num_params_trans_matrix = self.n_states* self.n_actions\n",
    "      num_params_Q_weights = self.n_states * self.n_mixtures\n",
    "      num_params_Q_means = self.n_states * self.n_mixtures * self.n_features\n",
    "      if self.cov_type == True:\n",
    "        #if Full type covariance\n",
    "        num_params_Q_covariances = self.n_states * self.n_mixtures* self.n_features* self.n_features\n",
    "      else:\n",
    "        #if Diag covariance\n",
    "        num_params_Q_covariances = self.n_states *self.n_mixtures* self.n_features\n",
    "        \n",
    "      start = 0\n",
    "      stop = num_params_trans_matrix\n",
    "      params_ = parameters[start:stop]\n",
    "      #print(\"params_\",params_)\n",
    "      P = np.zeros((self.n_actions, 2, 2))\n",
    "\n",
    "      # Fill P dynamically\n",
    "      for action in range(self.n_actions):\n",
    "          P[action, 0, 0] = params_[2 * action]  # Probability of staying in state 0\n",
    "          P[action, 0, 1] = 1 - params_[2 * action]  # Probability of moving to state 1\n",
    "          P[action, 1, 0] = params_[2 * action + 1]  # Probability of moving to state 0 from 1\n",
    "          P[action, 1, 1] = 1 - params_[2 * action + 1]  # Probability of staying in state 1\n",
    "          \n",
    "      start = num_params_trans_matrix\n",
    "      stop = num_params_trans_matrix+num_params_Q_weights\n",
    "      Q_weights = parameters[start:stop].reshape(self.n_states, self.n_mixtures)\n",
    "      start = stop\n",
    "      stop = start+num_params_Q_means\n",
    "      Q_means = parameters[start:stop].reshape(self.n_states, self.n_mixtures, self.n_features)\n",
    "      start = stop\n",
    "      stop = start+num_params_Q_covariances\n",
    "      if self.cov_type == True:\n",
    "        #if FULL cov\n",
    "        Q_covariances = parameters[start:stop].reshape(self.n_states, self.n_mixtures, self.n_features, self.n_features)\n",
    "      else:\n",
    "        #if Diag cov\n",
    "        variances = parameters[start:stop].reshape(self.n_states, self.n_mixtures, self.n_features)\n",
    "        # Initialize the covariance matrix with zeros\n",
    "        Q_covariances = np.zeros((self.n_states, self.n_mixtures, self.n_features, self.n_features))\n",
    "        # Fill the diagonal with variances\n",
    "        for state in range(self.n_states):\n",
    "          for mixture in range(self.n_mixtures):\n",
    "            np.fill_diagonal(Q_covariances[state, mixture], variances[state, mixture])\n",
    "      start = stop\n",
    "      ST = parameters[start:]\n",
    "\n",
    "      return P, Q_weights, Q_means, Q_covariances, ST\n",
    "\n",
    "   def constraint_eq(self,params):\n",
    "      num_params_trans_matrix = self.n_states * self.n_actions\n",
    "      num_params_Q_weights = self.n_states * self.n_mixtures\n",
    "      start = num_params_trans_matrix\n",
    "      stop = start+num_params_Q_weights\n",
    "      Q_weights = params[start:stop].reshape(self.n_states, self.n_mixtures)\n",
    "      # Constraints to ensure each row in the last dimension sums to 1\n",
    "      return np.concatenate([np.sum(Q_weights, axis=1).flatten() - 1])\n",
    "\n",
    "   def ll_dynamic(self, parameters, data, init_b0, init_b1):\n",
    "      P, Q_weights, Q_means, Q_covariances, ST = self.extract_parameters(parameters)\n",
    "      agent = HiddenSemiMarkovDDC(P, Q_weights,Q_means,Q_covariances, ST,self.n_actions)\n",
    "\n",
    "      def ll_one(sample_path, init_b0, init_b1):\n",
    "          # this function calculated log likelihood for one sample path\n",
    "          ll = 0\n",
    "          x0, x1 = init_b0, init_b1\n",
    "          #print(x)\n",
    "          for cell in sample_path:\n",
    "              a, z1,z2 = cell\n",
    "              z = np.array([z1,z2])\n",
    "              a = int(a)\n",
    "              #print('a,z',a,z)\n",
    "              sigma = max(agent.sigma(x0, x1, a, z), np.finfo(float).eps)\n",
    "              assert sigma > 0, f'probability = {sigma} negativity or zero ERROR!'\n",
    "              #assert sigma < 1, f'probability = {sigma} ERROR!'\n",
    "              ll += np.log(sigma)\n",
    "              x0, x1 = agent.update_belief(x0, x1, a, z)\n",
    "              #print(x0.shape,x1.shape)\n",
    "          return ll\n",
    "\n",
    "      ll = 0  # start calculating total log likelihood\n",
    "      for i, history in enumerate(data):  # histories w.r.t. i-th initial belief\n",
    "          ll += ll_one(history, init_b0, init_b1)\n",
    "          #print(i)\n",
    "\n",
    "      return -ll  # max: log-likelihood <=> min: -log-likelihood\n",
    "   \n",
    "   def get_belief(self,parameters, data, init_b0, init_b1):\n",
    "      P, Q_weights, Q_means, Q_covariances, ST = self.extract_parameters(parameters)\n",
    "      agent = HiddenSemiMarkovDDC(P, Q_weights,Q_means,Q_covariances, ST,self.n_actions)\n",
    "\n",
    "      def ll_one(sample_path, init_b0, init_b1):\n",
    "          # this function calculated log likelihood for one sample path\n",
    "          x0, x1 = init_b0, init_b1\n",
    "          dict_x0 = []\n",
    "          dict_x1 = []\n",
    "          dict_x0.append(sum(x0))\n",
    "          dict_x1.append(sum(x1))\n",
    "          for cell in sample_path:\n",
    "              a, z1,z2 = cell\n",
    "              z = np.array([z1,z2])\n",
    "              a = int(a)\n",
    "              sigma = max(agent.sigma(x0, x1, a, z), np.finfo(float).eps)\n",
    "              assert sigma > 0, f'probability = {sigma} negativity or zero ERROR!'\n",
    "              #assert sigma < 1, f'probability = {sigma} ERROR!'\n",
    "              x0, x1 = agent.update_belief(x0, x1, a, z)\n",
    "              dict_x0.append(sum(x0))\n",
    "              dict_x1.append(sum(x1))\n",
    "          return dict_x0,dict_x1\n",
    "\n",
    "      d_x0, d_x1 = [],[]\n",
    "      for i, history in enumerate(data):  # histories w.r.t. i-th initial belief\n",
    "          dict_x0,dict_x1 = ll_one(history, init_b0, init_b1)\n",
    "          d_x0.append(dict_x0)\n",
    "          d_x1.append(dict_x1)\n",
    "      return d_x0, d_x1\n",
    "   \n",
    "   def observation_sampler(self,agent,x0,x1,num_samples):\n",
    "      state = np.random.choice(agent.S, p=[sum(x0),sum(x1)])\n",
    "      mix = np.random.choice(agent.M, p=agent.Q_weights[state])\n",
    "      # Sample from the obs distribution\n",
    "      obs = multivariate_normal.rvs(mean=agent.Q_means[state, mix],\n",
    "                              cov=agent.Q_covariances[state, mix],\n",
    "                              size = num_samples)\n",
    "      return obs\n",
    "   \n",
    "   def compute_beliefs_and_sigmas(self,agent, num_samples=100):\n",
    "      # Create caches\n",
    "      belief_cache = {}\n",
    "      sigma_cache = {}\n",
    "\n",
    "      # Create the states\n",
    "      vertices = np.eye(90)  # 90, n_bel(or bstates)\n",
    "      vert_ = []\n",
    "      vert_sum = []\n",
    "      \n",
    "      for i in range(len(vertices)):\n",
    "          vert = vertices[i]\n",
    "          v1 = vert[:75]  # :75, n_bstates for first ST\n",
    "          v2 = vert[75:]  # 75:, n_bstates for second ST\n",
    "          vert_.append([v1, v2])\n",
    "          vert_sum.append([v1.sum(), v2.sum()])\n",
    "      \n",
    "      vert_sum = np.array(vert_sum)\n",
    "\n",
    "      # Store sigmas, beliefs\n",
    "      for x in range(len(vert_)):\n",
    "          for a in range(agent.A):\n",
    "              obs_MC = self.observation_sampler(agent,vert_[x][0], vert_[x][1], num_samples)\n",
    "              for i, z in enumerate(obs_MC):\n",
    "                  key = (tuple(vert_[x][0]) + tuple(vert_[x][1]), a, i)\n",
    "                  x_next0, x_next1 = agent.update_belief(vert_[x][0], vert_[x][1], a, z)\n",
    "                  sigma_cache[key] = agent.sigma(vert_[x][0], vert_[x][1], a, z)\n",
    "                  belief_cache[key] = np.concatenate((x_next0, x_next1))\n",
    "\n",
    "      # Get vectorized beliefs\n",
    "      keys = list(belief_cache.keys())\n",
    "      num_keys = len(keys)\n",
    "      array_shape = belief_cache[keys[0]].shape\n",
    "      \n",
    "      four_d_array = np.empty((num_keys, *array_shape))\n",
    "      for i, key in enumerate(keys):\n",
    "          four_d_array[i] = belief_cache[key]\n",
    "      \n",
    "      beliefs = four_d_array.reshape(90, self.n_actions, num_samples, 90)  # n_states, n_acts, n_obs, n_bel\n",
    "      \n",
    "      # Get vectorized sigmas\n",
    "      keys = list(sigma_cache.keys())\n",
    "      num_keys = len(keys)\n",
    "      array_shape = sigma_cache[keys[0]].shape\n",
    "      \n",
    "      four_d_array = np.empty((num_keys, *array_shape))\n",
    "      for i, key in enumerate(keys):\n",
    "          four_d_array[i] = sigma_cache[key]\n",
    "      \n",
    "      sigmas_before_norm = four_d_array.reshape(90, self.n_actions, num_samples)  # n_states, n_acts, n_obs\n",
    "      \n",
    "      # Normalize sigmas\n",
    "      sum_along_last_dim = np.sum(sigmas_before_norm, axis=2, keepdims=True)\n",
    "      sigmas = sigmas_before_norm / sum_along_last_dim\n",
    "      \n",
    "      self.vert_sum = vert_sum\n",
    "      self.beliefs = beliefs\n",
    "      self.sigmas = sigmas\n",
    "      return 0\n",
    "   \n",
    "   def value_function(self,reward_matrix,beta):\n",
    "      Q_val = self.vert_sum @ reward_matrix\n",
    "      V = logsumexp(Q_val, axis=1)\n",
    "      #count = 0\n",
    "      while True:\n",
    "          # Multiply using einsum\n",
    "          inter = np.einsum('ijkl,lm->ijkm', self.beliefs, Q_val)\n",
    "          EV = logsumexp(inter, axis=-1)\n",
    "          # Compute the dot product across the last dimension\n",
    "          future_ = np.einsum('ijk,ijk->ij', EV, self.sigmas)\n",
    "\n",
    "          Q_next = self.vert_sum @ reward_matrix + beta * future_\n",
    "          V_next = logsumexp(Q_next, axis=1)\n",
    "\n",
    "          condition = abs(V_next - V).sum()\n",
    "          Q_val = Q_next\n",
    "          V = V_next\n",
    "          if condition < 0.01:\n",
    "            break\n",
    "          #count+=1\n",
    "          #print('count',count,'condition',condition)\n",
    "\n",
    "      return Q_val, V\n",
    "   \n",
    "   def reward(self,w, B, C, s, a):\n",
    "      return w[s] * B[a] + (1 - w[s]) * (-C[a])\n",
    "   \n",
    "   def ll_ccp(self,parameters,agent, data, init_b0, init_b1, beta=0.9):\n",
    "    #r = np.zeros((2, self.n_actions))\n",
    "    # Fill the remaining elements from the vector\n",
    "    #r[0,0]=0\n",
    "    #r[1,0]=0\n",
    "    #r[:, 1:] = parameters.reshape(2, self.n_actions-1)\n",
    "    B = {0: 0,1: 4.1, 2: 5.3}\n",
    "    C = {0: 0,1: 3, 2: 6.2}\n",
    "    r = np.zeros((2, self.n_actions))\n",
    "    for s in [0, 1]:\n",
    "        for a in range(self.n_actions):\n",
    "            if a == 0:\n",
    "                r[s, a] = 0  # already set, explicitly\n",
    "            else:\n",
    "                r[s, a] = self.reward(parameters, B, C, s, a)\n",
    "    Q_val, _ = self.value_function(r, beta)\n",
    "    def ll_one(sample_path, init_b0, init_b1):\n",
    "        # this function calculated log likelihood for one sample path\n",
    "        ll = 0\n",
    "        x0, x1 = init_b0, init_b1\n",
    "        for cell in sample_path:\n",
    "            a, z1, z2 = cell\n",
    "            z = np.array([z1,z2])\n",
    "            a = int(a)\n",
    "            belief = np.concatenate((x0, x1))\n",
    "            Q_x = belief.flatten() @ Q_val\n",
    "            ccp = max(softmax(Q_x)[a], np.finfo(float).eps)\n",
    "            assert ccp > 0, f'prob = {ccp}, negativity or zero ERROR!'\n",
    "            ll += np.log(ccp)\n",
    "            x0, x1 = agent.update_belief(x0, x1, a, z)\n",
    "        return ll\n",
    "\n",
    "    ll = 0\n",
    "    for i, history in enumerate(data):  # histories w.r.t. i-th initial belief\n",
    "            ll += ll_one(history, init_b0, init_b1)\n",
    "\n",
    "    return -ll  # max: log-likelihood <=> min: -log-likelihood\n",
    "\n",
    "   def pretty_print_arrays(self,P, Q_means, Q_covariances, ST):\n",
    "      np.set_printoptions(precision=3, suppress=True)\n",
    "      print(\"State Transitions:\\n\", P, \"\\n\")\n",
    "      print(\"Means:\\n\", Q_means, \"\\n\")\n",
    "      print(\"Variance:\\n\", Q_covariances, \"\\n\")\n",
    "      print(\"Sojourn Time:\\n\", ST, \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2721cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/participant_bounds.json', 'r') as f:\n",
    "    participant_bounds = json.load(f)\n",
    "\n",
    "for pid in participant_bounds.keys():\n",
    "    participant_bounds[pid]['lb'] = np.array(participant_bounds[pid]['lb'])\n",
    "    participant_bounds[pid]['ub'] = np.array(participant_bounds[pid]['ub'])\n",
    "    participant_bounds[pid]['init'] = np.array(participant_bounds[pid]['init'])\n",
    "\n",
    "def fit_model_for_participant(participant_id, data_dir=\"Data\",seed=0):\n",
    "    print(f\"\\n=== Processing {participant_id} ===\")\n",
    "\n",
    "    # Load training data\n",
    "    try:\n",
    "        with open(f\"{data_dir}/{participant_id}_train_data.pkl\", \"rb\") as f:\n",
    "            train_data = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load training data for {participant_id}: {e}\")\n",
    "        return {\n",
    "            \"Participant\": participant_id,\n",
    "            \"LL\": None,\n",
    "            \"DynamicsParams\": None,\n",
    "            \"RewardParams\": None\n",
    "        }\n",
    "\n",
    "    # Fit dynamics model\n",
    "    print(\"Fitting the dynamics model...\")\n",
    "    estimate_d = estimate_dynamics(train_data)\n",
    "    constraints = {'type': 'eq', 'fun': estimate_d.constraint_eq, 'args': ()}\n",
    "    lb = participant_bounds[participant_id]['lb']\n",
    "    ub = participant_bounds[participant_id]['ub']\n",
    "    bnds = Bounds(lb=lb, ub=ub)\n",
    "    np.random.seed(seed)\n",
    "    init_b0 = np.random.rand(75)\n",
    "    init_b0 /= init_b0.sum()\n",
    "    init_b1 = np.zeros(15)\n",
    "    init_p = participant_bounds[participant_id]['init']\n",
    "\n",
    "    try:\n",
    "        res = minimize(\n",
    "            fun=estimate_d.ll_dynamic,\n",
    "            x0=init_p,\n",
    "            args=(train_data, init_b0, init_b1),\n",
    "            bounds=bnds,\n",
    "            constraints=constraints,\n",
    "            method='SLSQP',\n",
    "            options={'disp': 0}\n",
    "        )\n",
    "        param_estimates_1 = res.x\n",
    "        LL = res.fun\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {participant_id} (dynamic model failed): {e}\")\n",
    "        return {\n",
    "            \"Participant\": participant_id,\n",
    "            \"LL\": None,\n",
    "            \"DynamicsParams\": None,\n",
    "            \"RewardParams\": None\n",
    "        }\n",
    "\n",
    "    # Fit reward model\n",
    "    print(\"Fitting the reward model...\")\n",
    "    try:\n",
    "        P, Q_weights, Q_means, Q_covariances, ST = estimate_d.extract_parameters(param_estimates_1)\n",
    "        estimate_d.pretty_print_arrays(P, Q_means, Q_covariances, ST)\n",
    "        agent = HiddenSemiMarkovDDC(np.array(P), np.array(Q_weights), np.array(Q_means),\n",
    "                                    np.array(Q_covariances), np.array(ST))\n",
    "        estimate_d.compute_beliefs_and_sigmas(agent)\n",
    "        np.random.seed(seed)\n",
    "        b = np.random.random(75 + 15)\n",
    "        b /= b.sum()\n",
    "        init_b0 = b[:75]\n",
    "        init_b1 = b[75:]\n",
    "\n",
    "        x0 = np.random.random(2)\n",
    "        bnds2 = Bounds([0]*2, [1]*2)\n",
    "\n",
    "        res2 = minimize(\n",
    "            fun=estimate_d.ll_ccp,\n",
    "            x0=x0,\n",
    "            args=(agent, train_data, init_b0, init_b1),\n",
    "            bounds=bnds2,\n",
    "            method='SLSQP',\n",
    "            options={'disp': 0}\n",
    "        )\n",
    "        param_estimates_2 = res2.x\n",
    "        print(\"Estimated Rewards:\", param_estimates_2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {participant_id} (reward model failed): {e}\")\n",
    "        param_estimates_2 = None\n",
    "\n",
    "    return {\n",
    "        \"Participant\": participant_id,\n",
    "        \"LL\": LL,\n",
    "        \"DynamicsParams\": param_estimates_1,\n",
    "        \"RewardParams\": param_estimates_2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e236f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P1 ===\n",
      "Fitting the dynamics model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/scipy/optimize/_slsqp_py.py:435: RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\n",
      "  fx = wrapped_fun(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.618 0.382]\n",
      "  [0.904 0.096]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.172 0.828]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.706 -0.03 ]]\n",
      "\n",
      " [[32.389  0.204]]] \n",
      "\n",
      "Variance:\n",
      " [[[[0.684 0.   ]\n",
      "   [0.    0.108]]]\n",
      "\n",
      "\n",
      " [[[1.596 0.   ]\n",
      "   [0.    0.129]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.847 0.676] \n",
      "\n",
      "Estimated Rewards: [0.108 0.375]\n"
     ]
    }
   ],
   "source": [
    "res1 = fit_model_for_participant('P1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f2649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P2 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.726 0.274]\n",
      "  [1.    0.   ]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.84  0.16 ]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.665  0.803]]\n",
      "\n",
      " [[32.593  1.202]]] \n",
      "\n",
      "Variance:\n",
      " [[[[1.139 0.   ]\n",
      "   [0.    0.14 ]]]\n",
      "\n",
      "\n",
      " [[[1.274 0.   ]\n",
      "   [0.    0.179]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.767 0.769] \n",
      "\n",
      "Estimated Rewards: [0.234 0.455]\n"
     ]
    }
   ],
   "source": [
    "res2 = fit_model_for_participant('P2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c7ebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P3 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.001 0.999]\n",
      "  [1.    0.   ]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.028 0.972]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.498  0.213]]\n",
      "\n",
      " [[33.791  0.387]]] \n",
      "\n",
      "Variance:\n",
      " [[[[0.663 0.   ]\n",
      "   [0.    0.096]]]\n",
      "\n",
      "\n",
      " [[[1.504 0.   ]\n",
      "   [0.    0.155]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.66  0.724] \n",
      "\n",
      "Estimated Rewards: [0.298 0.396]\n"
     ]
    }
   ],
   "source": [
    "res3 = fit_model_for_participant('P3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "807d4873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P4 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.244 0.756]\n",
      "  [0.212 0.788]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.283 0.717]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.81  -0.223]]\n",
      "\n",
      " [[32.     0.022]]] \n",
      "\n",
      "Variance:\n",
      " [[[[1.398 0.   ]\n",
      "   [0.    0.086]]]\n",
      "\n",
      "\n",
      " [[[2.326 0.   ]\n",
      "   [0.    0.116]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.826 0.612] \n",
      "\n",
      "Estimated Rewards: [0.104 0.301]\n"
     ]
    }
   ],
   "source": [
    "res4 = fit_model_for_participant('P4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8222f1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P5 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.589 0.411]\n",
      "  [1.    0.   ]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.038 0.962]]] \n",
      "\n",
      "Means:\n",
      " [[[33.796 -0.309]]\n",
      "\n",
      " [[31.113 -0.201]]] \n",
      "\n",
      "Variance:\n",
      " [[[[0.97  0.   ]\n",
      "   [0.    0.168]]]\n",
      "\n",
      "\n",
      " [[[1.432 0.   ]\n",
      "   [0.    0.16 ]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.883 0.75 ] \n",
      "\n",
      "Estimated Rewards: [0.023 0.178]\n"
     ]
    }
   ],
   "source": [
    "res5 = fit_model_for_participant('P5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe04a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P6 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.001 0.999]\n",
      "  [1.    0.   ]]\n",
      "\n",
      " [[0.316 0.684]\n",
      "  [0.358 0.642]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.1   0.9  ]]] \n",
      "\n",
      "Means:\n",
      " [[[33.775  0.181]]\n",
      "\n",
      " [[32.5    0.263]]] \n",
      "\n",
      "Variance:\n",
      " [[[[0.815 0.   ]\n",
      "   [0.    0.112]]]\n",
      "\n",
      "\n",
      " [[[0.562 0.   ]\n",
      "   [0.    0.138]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.712 0.589] \n",
      "\n",
      "Estimated Rewards: [0.284 0.348]\n"
     ]
    }
   ],
   "source": [
    "res6 = fit_model_for_participant('P6',seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae540ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P7 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.757 0.243]\n",
      "  [0.329 0.671]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.182 0.818]]\n",
      "\n",
      " [[0.1   0.9  ]\n",
      "  [0.1   0.9  ]]] \n",
      "\n",
      "Means:\n",
      " [[[33.736 -0.077]]\n",
      "\n",
      " [[33.147  0.048]]] \n",
      "\n",
      "Variance:\n",
      " [[[[2.473 0.   ]\n",
      "   [0.    0.057]]]\n",
      "\n",
      "\n",
      " [[[1.434 0.   ]\n",
      "   [0.    0.066]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.709 0.836] \n",
      "\n",
      "Estimated Rewards: [0.279 0.39 ]\n"
     ]
    }
   ],
   "source": [
    "res7 = fit_model_for_participant('P7',seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a023eee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P8 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.109 0.891]\n",
      "  [1.    0.   ]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.129 0.871]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.497 -0.072]]\n",
      "\n",
      " [[33.85   0.225]]] \n",
      "\n",
      "Variance:\n",
      " [[[[0.783 0.   ]\n",
      "   [0.    0.077]]]\n",
      "\n",
      "\n",
      " [[[0.908 0.   ]\n",
      "   [0.    0.153]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.831 0.516] \n",
      "\n",
      "Estimated Rewards: [0.16  0.407]\n"
     ]
    }
   ],
   "source": [
    "res8 = fit_model_for_participant('P8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15782d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P9 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.436 0.564]\n",
      "  [0.881 0.119]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.647 0.353]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.73   0.075]]\n",
      "\n",
      " [[32.323  0.386]]] \n",
      "\n",
      "Variance:\n",
      " [[[[0.736 0.   ]\n",
      "   [0.    0.11 ]]]\n",
      "\n",
      "\n",
      " [[[0.786 0.   ]\n",
      "   [0.    0.15 ]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.859 0.699] \n",
      "\n",
      "Estimated Rewards: [0.146 0.454]\n"
     ]
    }
   ],
   "source": [
    "res9 = fit_model_for_participant('P9',seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d263860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P10 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.583 0.417]\n",
      "  [0.248 0.752]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.172 0.828]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.655  0.123]]\n",
      "\n",
      " [[33.333  0.401]]] \n",
      "\n",
      "Variance:\n",
      " [[[[0.846 0.   ]\n",
      "   [0.    0.139]]]\n",
      "\n",
      "\n",
      " [[[1.033 0.   ]\n",
      "   [0.    0.134]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.842 0.722] \n",
      "\n",
      "Estimated Rewards: [0.333 0.379]\n"
     ]
    }
   ],
   "source": [
    "res10 = fit_model_for_participant('P10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f41902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P11 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.605 0.395]\n",
      "  [0.896 0.104]]\n",
      "\n",
      " [[0.02  0.98 ]\n",
      "  [0.163 0.837]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.715  0.362]]\n",
      "\n",
      " [[32.704  0.629]]] \n",
      "\n",
      "Variance:\n",
      " [[[[2.394 0.   ]\n",
      "   [0.    0.085]]]\n",
      "\n",
      "\n",
      " [[[1.766 0.   ]\n",
      "   [0.    0.116]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.826 0.422] \n",
      "\n",
      "Estimated Rewards: [0.355 0.48 ]\n"
     ]
    }
   ],
   "source": [
    "res11 = fit_model_for_participant('P11',seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cea5482b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P12 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.497 0.503]\n",
      "  [0.811 0.189]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.612 0.388]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.759 -0.008]]\n",
      "\n",
      " [[32.4    0.298]]] \n",
      "\n",
      "Variance:\n",
      " [[[[1.192 0.   ]\n",
      "   [0.    0.156]]]\n",
      "\n",
      "\n",
      " [[[1.285 0.   ]\n",
      "   [0.    0.174]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.834 0.779] \n",
      "\n",
      "Estimated Rewards: [0.207 0.438]\n"
     ]
    }
   ],
   "source": [
    "res12 = fit_model_for_participant('P12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "083193b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P13 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.025 0.975]\n",
      "  [1.    0.   ]]\n",
      "\n",
      " [[0.266 0.734]\n",
      "  [0.7   0.3  ]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.762  0.097]]\n",
      "\n",
      " [[32.746  0.311]]] \n",
      "\n",
      "Variance:\n",
      " [[[[0.815 0.   ]\n",
      "   [0.    0.094]]]\n",
      "\n",
      "\n",
      " [[[0.876 0.   ]\n",
      "   [0.    0.148]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.859 0.945] \n",
      "\n",
      "Estimated Rewards: [0.138 0.352]\n"
     ]
    }
   ],
   "source": [
    "res13 = fit_model_for_participant('P13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0db61b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P14 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.119 0.881]\n",
      "  [0.643 0.357]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]\n",
      "\n",
      " [[0.034 0.966]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.577 -0.019]]\n",
      "\n",
      " [[33.555  0.241]]] \n",
      "\n",
      "Variance:\n",
      " [[[[0.724 0.   ]\n",
      "   [0.    0.061]]]\n",
      "\n",
      "\n",
      " [[[1.111 0.   ]\n",
      "   [0.    0.122]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.768 0.396] \n",
      "\n",
      "Estimated Rewards: [0.086 0.376]\n"
     ]
    }
   ],
   "source": [
    "res14 = fit_model_for_participant('P14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "536df24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P15 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.256 0.744]\n",
      "  [0.523 0.477]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.116 0.884]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.743  0.235]]\n",
      "\n",
      " [[32.905  0.358]]] \n",
      "\n",
      "Variance:\n",
      " [[[[1.54  0.   ]\n",
      "   [0.    0.095]]]\n",
      "\n",
      "\n",
      " [[[2.314 0.   ]\n",
      "   [0.    0.339]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.864 0.566] \n",
      "\n",
      "Estimated Rewards: [0.05  0.371]\n"
     ]
    }
   ],
   "source": [
    "res15 = fit_model_for_participant('P15',seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df9bf4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P16 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.361 0.639]\n",
      "  [0.9   0.1  ]]\n",
      "\n",
      " [[0.3   0.7  ]\n",
      "  [0.001 0.999]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.634  0.225]]\n",
      "\n",
      " [[33.398  0.516]]] \n",
      "\n",
      "Variance:\n",
      " [[[[1.379 0.   ]\n",
      "   [0.    0.136]]]\n",
      "\n",
      "\n",
      " [[[1.755 0.   ]\n",
      "   [0.    0.244]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.769 0.946] \n",
      "\n",
      "Estimated Rewards: [0.158 0.256]\n"
     ]
    }
   ],
   "source": [
    "res16 = fit_model_for_participant('P16',seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b7fdf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P17 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.326 0.674]\n",
      "  [0.514 0.486]]\n",
      "\n",
      " [[0.237 0.763]\n",
      "  [0.481 0.519]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[34.298  0.066]]\n",
      "\n",
      " [[31.316  0.165]]] \n",
      "\n",
      "Variance:\n",
      " [[[[1.763 0.   ]\n",
      "   [0.    0.072]]]\n",
      "\n",
      "\n",
      " [[[2.098 0.   ]\n",
      "   [0.    0.083]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.601 0.84 ] \n",
      "\n",
      "Estimated Rewards: [0.228 0.266]\n"
     ]
    }
   ],
   "source": [
    "res17 = fit_model_for_participant('P17',seed=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea7fb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing P18 ===\n",
      "Fitting the dynamics model...\n",
      "Fitting the reward model...\n",
      "State Transitions:\n",
      " [[[0.48  0.52 ]\n",
      "  [0.278 0.722]]\n",
      "\n",
      " [[0.001 0.999]\n",
      "  [0.176 0.824]]\n",
      "\n",
      " [[0.064 0.936]\n",
      "  [0.001 0.999]]] \n",
      "\n",
      "Means:\n",
      " [[[33.542 -0.007]]\n",
      "\n",
      " [[33.71   0.2  ]]] \n",
      "\n",
      "Variance:\n",
      " [[[[0.451 0.   ]\n",
      "   [0.    0.088]]]\n",
      "\n",
      "\n",
      " [[[0.764 0.   ]\n",
      "   [0.    0.122]]]] \n",
      "\n",
      "Sojourn Time:\n",
      " [0.883 0.311] \n",
      "\n",
      "Estimated Rewards: [0.216 0.367]\n"
     ]
    }
   ],
   "source": [
    "res18 = fit_model_for_participant('P18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d215a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing participant P1...\n",
      "Processing participant P2...\n",
      "Processing participant P3...\n",
      "Processing participant P4...\n",
      "Processing participant P5...\n",
      "Processing participant P6...\n",
      "Processing participant P7...\n",
      "Processing participant P8...\n",
      "Processing participant P9...\n",
      "Processing participant P10...\n",
      "Processing participant P11...\n",
      "Processing participant P12...\n",
      "Processing participant P13...\n",
      "Processing participant P14...\n",
      "Processing participant P15...\n",
      "Processing participant P16...\n",
      "Processing participant P17...\n",
      "Processing participant P18...\n",
      "        Event Type  Total Events POSMDP Model (% Detected) Baseline Rule (% Detected)\n",
      "Distraction Events            38                     97.4%                      68.4%\n",
      "Lane Offset Events            17                    100.0%                      70.6%\n",
      "\n",
      "Average Lead Time (in time steps): 1.30 seconds\n"
     ]
    }
   ],
   "source": [
    "def analyze_participants(participant_ids, mean_1_dict, input_dir=\"Data\"):\n",
    "    all_results = []\n",
    "\n",
    "    for pid in participant_ids:\n",
    "        print(f\"Processing participant {pid}...\")\n",
    "\n",
    "        # Load test data\n",
    "        with open(os.path.join(input_dir, f\"{pid}_test_data.pkl\"), \"rb\") as f:\n",
    "            test_data = pickle.load(f)\n",
    "\n",
    "        #initialize class\n",
    "        estimate_d = estimate_dynamics(test_data)\n",
    "        # Use pretrained values for participant\n",
    "\n",
    "        mean_1 = mean_1_dict[pid]\n",
    "\n",
    "        # Initialize beliefs\n",
    "        np.random.seed(4)\n",
    "        init_b0 = np.random.rand(75)\n",
    "        init_b0 /= init_b0.sum()\n",
    "\n",
    "        init_b1 = np.zeros(15)\n",
    "\n",
    "        # Estimate beliefs\n",
    "        b0, b1 = estimate_d.get_belief(mean_1, test_data, init_b0, init_b1)\n",
    "        b0 = [item for sublist in b0 for item in sublist]\n",
    "        b1 = [item for sublist in b1 for item in sublist]\n",
    "\n",
    "        # Load true data\n",
    "        df = pd.read_csv(os.path.join(input_dir, f\"{pid}_true_data.csv\"))\n",
    "        df['x0'] = b0\n",
    "        df['x1'] = b1\n",
    "\n",
    "        # Extract segments\n",
    "        segments = []\n",
    "        current_segment = []\n",
    "        for _, row in df.iterrows():\n",
    "            if row['distraction'] == 1:\n",
    "                current_segment.append([row['x1'], row['glances_greater_than_2_mean'], abs(row['LaneOffset'])])\n",
    "            else:\n",
    "                if current_segment:\n",
    "                    segments.append(current_segment)\n",
    "                    current_segment = []\n",
    "        if current_segment:\n",
    "            segments.append(current_segment)\n",
    "\n",
    "        segments_array = [np.array(seg) for seg in segments]\n",
    "\n",
    "        result = []\n",
    "        for segment in segments_array:\n",
    "            cond0 = np.any(segment[:, 0] >= 0.5)\n",
    "            cond1 = np.any(segment[:, 1] > 0)\n",
    "            cond2 = np.any(segment[:, 2] >= 0.9)\n",
    "            result.append([cond0, cond1, cond2])\n",
    "        result_array = np.array(result)\n",
    "\n",
    "        # Summary statistics\n",
    "        n_true_distractions = len(result_array)\n",
    "        n_predicted_distractions = int(np.sum(result_array[:, 0]))\n",
    "        n_threshold_distractions = int(np.sum(result_array[:, 1]))\n",
    "        n_true_lane_offsets = int(np.sum(result_array[:, 2]))\n",
    "        n_lane_model = int(np.sum(result_array[:, 0] & result_array[:, 2]))\n",
    "        n_lane_thresh = int(np.sum(result_array[:, 1] & result_array[:, 2]))\n",
    "\n",
    "        # Lead time analysis\n",
    "        lead_times = []\n",
    "        for segment in segments_array:\n",
    "            col0_pred = np.where(segment[:, 0] >= 0.5)[0]\n",
    "            col1_pred = np.where(segment[:, 1] > 0)[0]\n",
    "            if len(col0_pred) > 0 and len(col1_pred) > 0:\n",
    "                idx_0 = col0_pred[0]\n",
    "                idx_1 = col1_pred[0]\n",
    "                lead_times.append(idx_1 - idx_0)\n",
    "\n",
    "        if lead_times:\n",
    "            lead_time_mean = np.mean(lead_times)\n",
    "        else:\n",
    "            lead_time_mean = None\n",
    "\n",
    "        all_results.append({\n",
    "            \"Participant\": pid,\n",
    "            \"TrueDistractionEvents\": n_true_distractions,\n",
    "            \"ModelDistractionEvents\": n_predicted_distractions,\n",
    "            \"ThresholdDistractionEvents\": n_threshold_distractions,\n",
    "            \"TrueLaneOffsetEvents\": n_true_lane_offsets,\n",
    "            \"ModelLaneOffsetEvents\": n_lane_model,\n",
    "            \"ThresholdLaneOffsetEvents\": n_lane_thresh,\n",
    "            \"LeadTimeMean\": lead_time_mean,\n",
    "        })\n",
    "\n",
    "    # Compile into DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    return results_df\n",
    "\n",
    "with open(\"Data/pre_trained_values.pkl\", \"rb\") as f:\n",
    "    pre_trained_dict = pickle.load(f)\n",
    "participant_ids = [f\"P{i}\" for i in range(1, 19)]\n",
    "results_df = analyze_participants(participant_ids, pre_trained_dict)\n",
    "\n",
    "def print_detection_summary(results_df):\n",
    "    # Aggregate totals\n",
    "    total_distraction = results_df[\"TrueDistractionEvents\"].sum()\n",
    "    model_distraction = results_df[\"ModelDistractionEvents\"].sum()\n",
    "    rule_distraction = results_df[\"ThresholdDistractionEvents\"].sum()\n",
    "\n",
    "    total_lane = results_df[\"TrueLaneOffsetEvents\"].sum()\n",
    "    model_lane = results_df[\"ModelLaneOffsetEvents\"].sum()\n",
    "    rule_lane = results_df[\"ThresholdLaneOffsetEvents\"].sum()\n",
    "\n",
    "    # Average lead time (excluding NaNs)\n",
    "    avg_lead_time = results_df[\"LeadTimeMean\"].dropna().mean()\n",
    "\n",
    "    # Create summary table\n",
    "    summary = pd.DataFrame([\n",
    "        {\n",
    "            \"Event Type\": \"Distraction Events\",\n",
    "            \"Total Events\": total_distraction,\n",
    "            \"POSMDP Model (% Detected)\": f\"{(model_distraction / total_distraction * 100):.1f}%\",\n",
    "            \"Baseline Rule (% Detected)\": f\"{(rule_distraction / total_distraction * 100):.1f}%\"\n",
    "        },\n",
    "        {\n",
    "            \"Event Type\": \"Lane Offset Events\",\n",
    "            \"Total Events\": total_lane,\n",
    "            \"POSMDP Model (% Detected)\": f\"{(model_lane / total_lane * 100):.1f}%\",\n",
    "            \"Baseline Rule (% Detected)\": f\"{(rule_lane / total_lane * 100):.1f}%\"\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    print(summary.to_string(index=False))\n",
    "    print(f\"\\nAverage Lead Time (in time steps): {avg_lead_time:.2f} seconds\")\n",
    "    \n",
    "print_detection_summary(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
